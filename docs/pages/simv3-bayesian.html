---
layout: AURA-MF
title: SimV3 Bayesian Optimization & Surrogate Modeling
---

    <main>

        <div class="executive-summary">
            <h2>Executive Summary</h2>
            <p>SimV3 will introduce <strong>Gaussian Process (GP) surrogates</strong> and <strong>Bayesian optimization</strong> 
            to enable efficient design space exploration and parameter identification for photovoltaic thermal systems. By constructing 
            probabilistic emulators of the expensive SimV1/SimV2 solvers, SimV3 hopes to reduce the cost of optimization from thousands of 
            HF evaluations to fewer than <strong>100 strategically selected queries</strong>, achieving global optima with quantified 
            uncertainty bounds.</p>
        </div>

        <h2>Gaussian Process Regression Framework</h2>

        <h3>Probabilistic Model</h3>
        
        <p>A Gaussian Process defines a distribution over functions $f: \mathcal{X} \rightarrow \mathbb{R}$, specified by a mean 
        function $m(\mathbf{x})$ and covariance kernel $k(\mathbf{x}, \mathbf{x}')$:</p>

        <p style="text-align: center; margin: 2rem 0;">
            $$f(\mathbf{x}) \sim \mathcal{GP}\left(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}')\right)$$
        </p>

        <p>For PV thermal modeling, $\mathbf{x} \in \mathbb{R}^d$ represents design parameters (e.g., encapsulant thickness, 
        thermal conductivity, surface emissivity), and $f(\mathbf{x})$ is the output of interest (e.g., peak temperature, 
        power conversion efficiency).</p>

        <h3>Posterior Predictive Distribution</h3>

        <p>Given training data $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$ where $y_i = f(\mathbf{x}_i) + \epsilon_i$, 
        $\epsilon_i \sim \mathcal{N}(0, \sigma_n^2)$, the posterior GP at test point $\mathbf{x}_*$ is:</p>

        <p style="text-align: center; margin: 2rem 0;">
            $$\begin{aligned}
            \mu(\mathbf{x}_*) &= \mathbf{k}_*^T (\mathbf{K} + \sigma_n^2 \mathbf{I})^{-1} \mathbf{y} \\[10pt]
            \sigma^2(\mathbf{x}_*) &= k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^T (\mathbf{K} + \sigma_n^2 \mathbf{I})^{-1} \mathbf{k}_*
            \end{aligned}$$
        </p>

        <p>The posterior variance $\sigma^2(\mathbf{x}_*)$ quantifies <strong>epistemic uncertainty</strong> (reducible via 
        additional data), distinct from aleatoric noise $\sigma_n^2$.</p>

        <h2>Kernel Design for PV Systems</h2>

        <h3>Matérn 5/2 Kernel</h3>

        <p>SimV3 will employ the Matérn 5/2 kernel for its balance between smoothness and flexibility:</p>

        <p style="text-align: center; margin: 2rem 0;">
            $$k_{\text{M52}}(\mathbf{x}, \mathbf{x}') = \sigma_f^2 \left(1 + \frac{\sqrt{5}r}{\ell} + \frac{5r^2}{3\ell^2}\right) 
            \exp\left(-\frac{\sqrt{5}r}{\ell}\right)$$
        </p>

        <p>where $r = \|\mathbf{x} - \mathbf{x}'\|_2$, $\ell > 0$ is the length-scale, and $\sigma_f^2$ is signal variance.</p>

        <div class="theory-box">
            <p><strong>Rationale:</strong> Matérn 5/2 produces twice-differentiable samples (vs. infinitely differentiable for SE), 
            making it more robust to discontinuities in PV response surfaces (e.g., bypass diode activation).</p>
        </div>

        <h2>Bayesian Optimization Algorithm</h2>

        <h3>Expected Improvement Acquisition Function</h3>

        <p>Bayesian Optimization (BO) sequentially selects query points by maximizing an <strong>acquisition function</strong> 
        $\alpha(\mathbf{x})$ that balances exploration (high uncertainty) and exploitation (high predicted performance).</p>

        <p><strong>Expected Improvement</strong> over current best $f^+ = \max_{i \leq n} y_i$:</p>

        <p style="text-align: center; margin: 2rem 0;">
            $$\text{EI}(\mathbf{x}) = \mathbb{E}\left[\max(0, f(\mathbf{x}) - f^+)\right] = 
            (\mu(\mathbf{x}) - f^+) \Phi(Z) + \sigma(\mathbf{x}) \phi(Z)$$
        </p>

        <p>where $Z = \frac{\mu(\mathbf{x}) - f^+}{\sigma(\mathbf{x})}$, $\Phi(\cdot)$ is the CDF and $\phi(\cdot)$ is the 
        PDF of standard normal distribution.</p>

        <p>At each iteration $t$, solve:</p>

        <p style="text-align: center; margin: 2rem 0;">
            $$\mathbf{x}_{t+1} = \underset{\mathbf{x} \in \mathcal{X}}{\arg\max} \, \text{EI}(\mathbf{x} \mid \mathcal{D}_t)$$
        </p>

        <!--<h2>Case Study: Encapsulant Thickness Optimization</h2>

        <h3>Problem Setup</h3>

        <p><strong>Objective:</strong> Minimize peak temperature $T_{\max}$ while maintaining structural integrity</p>
        <p><strong>Design variable:</strong> Ethylene-vinyl acetate (EVA) thickness $h \in [0.3, 1.0]$ mm</p>

        <div class="theory-box">
            <h4>Constraints:</h4>
            <ul>
                <li>$h \geq 0.4$ mm (manufacturing limit)</li>
                <li>$\text{Cost}(h) \leq 1.2 \times \text{Cost}_{\text{baseline}}$</li>
            </ul>
            <h4>Fixed parameters:</h4>
            <p>$T_\infty = 298$ K, $G = 1000$ W/m², $u_\infty = 1.0$ m/s</p>
        </div>

        <h3>Optimization Trajectory</h3>

        <table>
            <thead>
                <tr>
                    <th>Iteration</th>
                    <th>$h$ [mm]</th>
                    <th>$T_{\max}$ [K]</th>
                    <th>EI</th>
                    <th>HF Calls</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1 (random)</td><td>0.55</td><td>322.7</td><td>—</td><td>1</td></tr>
                <tr><td>2</td><td>0.43</td><td>324.1</td><td>0.82</td><td>2</td></tr>
                <tr><td>5</td><td>0.68</td><td>320.5</td><td>0.34</td><td>5</td></tr>
                <tr style="background: #ecfdf5;"><td>10</td><td>0.72</td>
                    <td style="color: var(--primary); font-weight: 600;">319.8</td><td>0.09</td><td>10</td></tr>
                <tr><td>15</td><td>0.71</td><td>319.9</td><td>0.003</td><td>10</td></tr>
            </tbody>
        </table>

        <p><strong>Converged solution:</strong> $h^* = 0.72$ mm → $T_{\max} = 319.8$ K</p>
        <p><strong>Validation:</strong> SimV1 HF evaluation at $h^*$ yields $T_{\max} = 319.7$ K (0.03% error)</p>
        <p><strong>Speedup:</strong> 10 HF calls (16 min) vs. 200+ for grid search (50+ hours)</p>

        <h2>Sensitivity Analysis via Sobol Indices</h2>

        <h3>First-Order Sobol Index</h3>

        <p>Quantifies the fraction of output variance due to parameter $x_i$ alone:</p>

        <p style="text-align: center; margin: 2rem 0;">
            $$S_i = \frac{\mathbb{V}_{x_i}\left[\mathbb{E}_{x_{\sim i}}[f(\mathbf{x}) \mid x_i]\right]}{\mathbb{V}[f(\mathbf{x})]}$$
        </p>

        <h3>Total-Order Sobol Index</h3>

        <p>Captures all interactions involving $x_i$:</p>

        <p style="text-align: center; margin: 2rem 0;">
            $$S_i^{\text{total}} = 1 - \frac{\mathbb{V}_{x_{\sim i}}\left[\mathbb{E}_{x_i}[f(\mathbf{x}) \mid x_{\sim i}]\right]}
            {\mathbb{V}[f(\mathbf{x})]}$$
        </p>

        <p><strong>Interpretation:</strong> If $S_i^{\text{total}} \approx 0$, parameter $x_i$ can be fixed without loss of accuracy.</p>

        <div class="warning-box">
            <p><strong>Cost:</strong> Requires $N(2d+2)$ function evaluations. With GP surrogate, $N=10^4$ is feasible 
            (milliseconds), whereas direct SimV1 evaluation would be prohibitive.</p>
        </div>

        <h2>Hyperparameter Tuning</h2>

        <h3>Marginal Likelihood Maximization</h3>

        <p>Kernel hyperparameters $\boldsymbol{\theta} = \{\ell_1, \ldots, \ell_d, \sigma_f, \sigma_n\}$ are learned by 
        maximizing the log marginal likelihood:</p>

        <p style="text-align: center; margin: 2rem 0;">
            $$\log p(\mathbf{y} \mid \mathbf{X}, \boldsymbol{\theta}) = -\frac{1}{2} \mathbf{y}^T \mathbf{K}_\theta^{-1} 
            \mathbf{y} - \frac{1}{2} \log |\mathbf{K}_\theta| - \frac{n}{2} \log(2\pi)$$
        </p>

        <p>SimV3 uses L-BFGS with box constraints: $\ell_i \in [10^{-3}, 10^3]$, $\sigma_f \in [10^{-6}, 10^2]$, 
        $\sigma_n \in [10^{-8}, 1]$.</p>

        <h2>Key Contributions</h2>

        <ol>
            <li><strong>First application of multi-fidelity Bayesian optimization</strong> to PV thermal systems 
                (achieves global optima in <100 HF evaluations)</li>
            <li><strong>Rigorous uncertainty quantification</strong> for design decisions (credible intervals guide risk-informed choices)</li>
            <li><strong>Open-source implementation</strong> (GPyTorch + BoTorch, GPU-accelerated)</li>
        </ol>-->

        <h2>References</h2>

        <ol>
            <li>Rasmussen, C. E., & Williams, C. K. I. (2006). <em>Gaussian Processes for Machine Learning</em>. MIT Press.</li>
            <li>Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. 
                <em>NeurIPS</em>, 25.</li>
            <li>Forrester, A. I., Sóbester, A., & Keane, A. J. (2008). <em>Engineering Design via Surrogate Modelling</em>. Wiley.</li>
            <li>Saltelli, A., et al. (2010). Variance based sensitivity analysis of model output. 
                <em>Computer Physics Communications</em>, 181(2), 259-270.</li>
        </ol>

    </main>
